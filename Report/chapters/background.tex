SPA introduces several abstractions for representing sparsity information over 
tensors.

\subsection{Propagation Strategy}

SPA works by propagating sparsity information in multiple directions during 
computation of tensor operations. The main directions are:

\begin{enumerate}
    \item Forward propagation: where information contained in the input tensors
        $A$ and $B$, will constrain the output tensor $C = A \times B$.
    \item Backward propagation: where the information contained in the output
        tensor $C$ will constrain the input tensors $A$ and $B$.
    \item Lateral propagation: where the information contained in either input 
        $A$ and $B$, will contrain the other.
\end{enumerate}

\subsection{Sparsity Vectors}

Sparsity vectors are data structures that SPA uses to represent the sparsity 
information of a tensor. Sparsity vectors are bitmaps per dimension of a tensor, 
where a single bit is set to 0 if the corresponding slice is populated by zero 
values, and 1 otherwise.

\subsection{Semirings}

SPA is formulated over the algebraic structure of semirings, and so it supports
operations that have absorbing and identity elements, such as multiplication 
and addition. Currently, SPA only supports the semirings of addition and 
multiplication, but it can be extended to support more semirings.

\subsection{Transfer Functions}

Transfer functions are the vehicles that drive sparsity information of tensors 
across operations via the sparsity vectors. These functions specify how the 
sparsity vectors of the input and output tensors constrain each other, based 
on the propagation strategy employed, and capture how sparse slices flow through
the computational graph.

\subsection{Propagation Order}

Due to data dependencies in a computational graph, when the order of propagations 
is applied in the specific sequence of forward, lateral, and backward, the analysis 
converges to a fixed point, provided that the graph is acyclic.




